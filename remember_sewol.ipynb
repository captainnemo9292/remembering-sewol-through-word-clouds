{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2019\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "k=0\n",
    "\n",
    "list=[]\n",
    "for i in range(0,15000):\n",
    "    list.append(i)\n",
    "data = pd.DataFrame(list, columns = ['Text']) \n",
    "\n",
    "req = Request(\"https://mobile.twitter.com/search?q=%EC%84%B8%EC%9B%94%ED%98%B8%20since%3A2019-01-01%20until%3A2019-04-15&src=typd&f=live\")\n",
    "html_page = urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "    data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "    k=k+1\n",
    "    \n",
    "next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "\n",
    "data\n",
    "\n",
    "data.to_csv(\"remember_sewol_2019.csv\")\n",
    "\n",
    "while k<10000:\n",
    "    \n",
    "    url = \"https://mobile.twitter.com\" + str(next_link)\n",
    "    req = Request(url)\n",
    "    html_page = urlopen(req)\n",
    "\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "        data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "        k=k+1\n",
    "        print(k)\n",
    "        print(i.get_text().replace('\\n',' '))\n",
    "        \n",
    "    next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "    data.to_csv(\"remember_sewol_2019.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2018\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "k=0\n",
    "\n",
    "list=[]\n",
    "for i in range(0,15000):\n",
    "    list.append(i)\n",
    "data = pd.DataFrame(list, columns = ['Text']) \n",
    "\n",
    "req = Request(\"https://mobile.twitter.com/search?q=%EC%84%B8%EC%9B%94%ED%98%B8%20since%3A2018-01-01%20until%3A2018-12-30&src=typd&f=live\")\n",
    "html_page = urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "    data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "    k=k+1\n",
    "    \n",
    "next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "\n",
    "data\n",
    "\n",
    "data.to_csv(\"remember_sewol_2018.csv\")\n",
    "\n",
    "while k<10000:\n",
    "    \n",
    "    url = \"https://mobile.twitter.com\" + str(next_link)\n",
    "    req = Request(url)\n",
    "    html_page = urlopen(req)\n",
    "\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "        data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "        k=k+1\n",
    "        print(k)\n",
    "        print(i.get_text().replace('\\n',' '))\n",
    "        \n",
    "    next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "    data.to_csv(\"remember_sewol_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "k=0\n",
    "\n",
    "list=[]\n",
    "for i in range(0,15000):\n",
    "    list.append(i)\n",
    "data = pd.DataFrame(list, columns = ['Text']) \n",
    "\n",
    "req = Request(\"https://mobile.twitter.com/search?q=%EC%84%B8%EC%9B%94%ED%98%B8%20since%3A2017-01-01%20until%3A2017-12-30&src=typd&f=live\")\n",
    "html_page = urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "    data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "    k=k+1\n",
    "    \n",
    "next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "\n",
    "data\n",
    "\n",
    "data.to_csv(\"remember_sewol_2017.csv\")\n",
    "\n",
    "while k<10000:\n",
    "    \n",
    "    url = \"https://mobile.twitter.com\" + str(next_link)\n",
    "    req = Request(url)\n",
    "    html_page = urlopen(req)\n",
    "\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "        data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "        k=k+1\n",
    "        print(k)\n",
    "        print(i.get_text().replace('\\n',' '))\n",
    "        \n",
    "    next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "    data.to_csv(\"remember_sewol_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2016\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "k=0\n",
    "\n",
    "list=[]\n",
    "for i in range(0,15000):\n",
    "    list.append(i)\n",
    "data = pd.DataFrame(list, columns = ['Text']) \n",
    "\n",
    "req = Request(\"https://mobile.twitter.com/search?q=%EC%84%B8%EC%9B%94%ED%98%B8%20since%3A2016-01-01%20until%3A2016-12-30&src=typd&f=live\")\n",
    "html_page = urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "    data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "    k=k+1\n",
    "    \n",
    "next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "\n",
    "data\n",
    "\n",
    "data.to_csv(\"remember_sewol_2016.csv\")\n",
    "\n",
    "while k<10000:\n",
    "    \n",
    "    url = \"https://mobile.twitter.com\" + str(next_link)\n",
    "    req = Request(url)\n",
    "    html_page = urlopen(req)\n",
    "\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "        data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "        k=k+1\n",
    "        print(k)\n",
    "        print(i.get_text().replace('\\n',' '))\n",
    "        \n",
    "    next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "    data.to_csv(\"remember_sewol_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2015\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "k=0\n",
    "\n",
    "list=[]\n",
    "for i in range(0,15000):\n",
    "    list.append(i)\n",
    "data = pd.DataFrame(list, columns = ['Text']) \n",
    "\n",
    "req = Request(\"https://mobile.twitter.com/search?q=%EC%84%B8%EC%9B%94%ED%98%B8%20since%3A2015-01-01%20until%3A2015-12-30&src=typd&f=live\")\n",
    "html_page = urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "    data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "    k=k+1\n",
    "    \n",
    "next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "\n",
    "data\n",
    "\n",
    "data.to_csv(\"remember_sewol_2015.csv\")\n",
    "\n",
    "while k<10000:\n",
    "    \n",
    "    url = \"https://mobile.twitter.com\" + str(next_link)\n",
    "    req = Request(url)\n",
    "    html_page = urlopen(req)\n",
    "\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "        data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "        k=k+1\n",
    "        print(k)\n",
    "        print(i.get_text().replace('\\n',' '))\n",
    "        \n",
    "    next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "    data.to_csv(\"remember_sewol_2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2014\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "k=0\n",
    "\n",
    "list=[]\n",
    "for i in range(0,15000):\n",
    "    list.append(i)\n",
    "data = pd.DataFrame(list, columns = ['Text']) \n",
    "\n",
    "req = Request(\"https://mobile.twitter.com/search?q=%EC%84%B8%EC%9B%94%ED%98%B8%20since%3A2014-01-01%20until%3A2014-12-30&src=typd&f=live\")\n",
    "html_page = urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "    data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "    k=k+1\n",
    "    \n",
    "next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "\n",
    "data\n",
    "\n",
    "data.to_csv(\"remember_sewol_2014.csv\")\n",
    "\n",
    "while k<10000:\n",
    "    \n",
    "    url = \"https://mobile.twitter.com\" + str(next_link)\n",
    "    req = Request(url)\n",
    "    html_page = urlopen(req)\n",
    "\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all(\"div\",{\"class\": \"tweet-text\"}):\n",
    "        data['Text'][k] = i.get_text().replace('\\n',' ')\n",
    "        k=k+1\n",
    "        print(k)\n",
    "        print(i.get_text().replace('\\n',' '))\n",
    "        \n",
    "    next_link = soup.find(\"div\",{\"class\":\"w-button-more\"}).find(\"a\").get(\"href\")\n",
    "    data.to_csv(\"remember_sewol_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.read_csv(\"remember_sewol_2019.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
